# Project 07

# ğŸ§  Feature Selection and Dimensionality Reduction on TinyMNIST Dataset

This project applies **feature selection** and **dimensionality reduction** techniques on the TinyMNIST dataset using:

- âœ… Principal Component Analysis (PCA)
- âœ… Forward Selection
- âœ… Backward Elimination

The goal is to reduce dimensionality while maintaining high classification performance using a **NaÃ¯ve Bayes classifier**.

---

## ğŸ“‚ Dataset

The dataset used is a small version of MNIST (TinyMNIST):

- `trainData.csv`: Training features  
- `trainLabels.csv`: Training labels  
- `testData.csv`: Testing features  
- `testLabels.csv`: Testing labels

Each image is flattened into a vector. Features are pixel intensities.

---

## ğŸ§ª Algorithms Implemented

### ğŸ”¹ 1. Principal Component Analysis (PCA)

**Goal:** Reduce dimensionality by finding orthogonal directions (principal components) that capture maximum variance.

**Steps:**
1. Center the data.
2. Compute the covariance matrix.
3. Perform eigen-decomposition.
4. Sort eigenvalues/eigenvectors in descending order.
5. Retain top-`k` eigenvectors that preserve 95% of variance.

ğŸ“Š The explained variance is plotted to visualize how many components are required to retain 95% of information.

### ğŸ”¹ 2. Forward Feature Selection

**Goal:** Start with no features and iteratively add the one that improves performance the most.

**Process:**
- Use `GaussianNB` for evaluation.
- Continue until no further improvement in CCR (Classification Correctness Rate).

### ğŸ”¹ 3. Backward Feature Elimination

**Goal:** Start with all features and remove the one that least affects or improves the model.

**Process:**
- Evaluate accuracy by removing each feature.
- Eliminate the one whose removal increases or least reduces the CCR.
- Repeat until no improvements can be made.

---

## âš™ï¸ Dependencies

Install required packages using:

```bash
pip install numpy matplotlib scikit-learn
```

---

## ğŸ“Š Evaluation Metric

The classifier used is `Gaussian Naive Bayes (GNB)`, and the evaluation metric is **Accuracy (CCR)**.

---

## ğŸ“ˆ Results

- PCA retained ~95% of variance with only `62` components.
- Both forward and backward selection successfully identified a reduced feature set with similar or better CCR.
- Performance comparison of PCA vs. wrapper methods was plotted for analysis.

---

## ğŸ“Œ Visualizations

- **Eigenvalue Spectrum** (PCA)
- **Cumulative Explained Variance**
- **CCR vs Number of Features** for Forward and Backward Selection

Example:

```python
plt.plot(range(1, len(forward_accuracies) + 1), forward_accuracies)
plt.xlabel('Number of Selected Features')
plt.ylabel('CCR')
```

---

## ğŸ§ª How to Run

1. Place your dataset files in the same folder.
2. Run the Python script to execute PCA, forward selection, and backward elimination.
3. View results and plots.

---

## ğŸ“ File Structure

```
07 pro/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ trainData.csv
â”‚   â”œâ”€â”€ trainLabels.csv
â”‚   â”œâ”€â”€ testData.csv
â”‚   â””â”€â”€ testLabels.csv
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main.ipynb
â”œâ”€â”€ images/
â”œâ”€â”€ README.md             # This file
```

---

## âœï¸ Author

- ğŸ‘¤ Mohammad Javad
- ğŸ“§ mj.mousavi@ut.ac.ir  
- ğŸ“ [GitHub Profile](https://github.com/mjmousavi97)

---

## ğŸ“„ License

This project is licensed under the MIT License.
