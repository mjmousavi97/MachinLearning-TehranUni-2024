{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbf82d70",
   "metadata": {},
   "source": [
    "# Backward Elimination Algorithm for Feature Selection\n",
    "\n",
    "**Backward Elimination** is a classic **wrapper** method used for feature selection in machine learning. The main idea is to start with all available features and iteratively remove the least significant feature at each step until the optimal subset of features is obtained.\n",
    "\n",
    "## How Backward Elimination Works:\n",
    "\n",
    "1. **Start with all features** in the dataset.\n",
    "2. **Train the model** (e.g., Na√Øve Bayes classifier) using the current set of features.\n",
    "3. **Evaluate the model's performance** using a chosen metric (such as accuracy).\n",
    "4. **Identify the least significant feature** (usually the one whose removal improves or least degrades performance).\n",
    "5. **Remove that feature** from the feature set.\n",
    "6. **Repeat steps 2-5** until removing more features does not improve the model or a stopping criterion is met (e.g., a predefined number of features or performance threshold).\n",
    "\n",
    "## Advantages:\n",
    "- Considers feature interactions because it evaluates subsets with the actual classifier.\n",
    "- Often leads to a smaller, more relevant feature set.\n",
    "\n",
    "## Disadvantages:\n",
    "- Computationally expensive because it requires training the model multiple times.\n",
    "- Can get stuck in local optima.\n",
    "\n",
    "In this project, we will implement **Backward Elimination** manually (without using any ready-made libraries) to select features on the TinyMNIST dataset. For classification, we will use the Na√Øve Bayes optimal classifier from existing packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93760470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2484e130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 5000\n",
      "Number of features: 196\n"
     ]
    }
   ],
   "source": [
    "train_data = np.loadtxt('/mnt/e/Term 3/Machin-Learning/Projects/07 pro/data/trainData.csv', delimiter=',', dtype=np.float32)\n",
    "train_labels = np.loadtxt('/mnt/e/Term 3/Machin-Learning/Projects/07 pro/data/trainLabels.csv', delimiter=',', dtype=np.float32)\n",
    "test_data = np.loadtxt('/mnt/e/Term 3/Machin-Learning/Projects/07 pro/data/testData.csv', delimiter=',', dtype=np.float32)\n",
    "test_labels = np.loadtxt('/mnt/e/Term 3/Machin-Learning/Projects/07 pro/data/testLabels.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "print(f'Number of training samples: {train_data.shape[0]}')\n",
    "print(f'Number of features: {train_data.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0481cc",
   "metadata": {},
   "source": [
    "### üîç Variance Threshold Feature Selection\n",
    "\n",
    "In this section, we apply **feature selection** using the `VarianceThreshold` method from `sklearn.feature_selection`.\n",
    "\n",
    "The idea is to remove features (columns) that have very low variance, as they provide little to no information for classification.\n",
    "\n",
    "#### Steps:\n",
    "1. **Concatenate** the training and test data vertically using `np.vstack()` to ensure that feature selection is performed globally across the entire dataset.\n",
    "2. **Apply VarianceThreshold** with a threshold of `0.09` (which is `0.90 * (1 - 0.90)`). This means:\n",
    "   - Any feature with variance **less than 0.09** will be removed.\n",
    "   - This threshold assumes binary data, where maximum variance is `0.25` (for 50-50 split), so 0.09 is a reasonable cutoff.\n",
    "3. **Transform** the combined data using the fitted selector, which returns only the selected features.\n",
    "4. **Split** the filtered data back into training and testing sets.\n",
    "\n",
    "This helps in reducing dimensionality and keeping only the most informative features for classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ec7286e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Samples: 5000 , Test Data Samples 2500 , Feature Size(after feature-selection): 62\n"
     ]
    }
   ],
   "source": [
    "all_data = np.vstack((train_data, test_data))\n",
    "var_selctor = VarianceThreshold(threshold=0.09)\n",
    "all_data = var_selctor.fit_transform(X=all_data)\n",
    "\n",
    "train_data_with_sel = all_data[:train_data.shape[0]]\n",
    "test_data_with_sel = all_data[train_data.shape[0]:]\n",
    "\n",
    "tr_samples_size, feature_size = train_data_with_sel.shape\n",
    "te_samples_size, _ = test_data_with_sel.shape\n",
    "\n",
    "print('Train Data Samples:',tr_samples_size,\n",
    "      ', Test Data Samples',te_samples_size,\n",
    "      ', Feature Size(after feature-selection):', feature_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9087c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_elimination(train_data, train_labels, test_data, test_labels):\n",
    "    selected_features = list(range(train_data.shape[1]))\n",
    "    accuracies = []\n",
    "    num_features = []\n",
    "    model = GaussianNB()\n",
    "    model.fit(train_data, train_labels)\n",
    "    predictions = model.predict(test_data)\n",
    "    acc = accuracy_score(y_true=test_labels, y_pred=predictions)\n",
    "    num_features.append(train_data.shape[1])\n",
    "    best_accuracy = acc\n",
    "    accuracies.append(acc)\n",
    "\n",
    "    while len(selected_features) > 1:\n",
    "        feature_to_remove = None\n",
    "        \n",
    "        for feature in selected_features:\n",
    "            candidate_features = [f for f in selected_features if f != feature]\n",
    "            model = GaussianNB()\n",
    "            model.fit(train_data[:, candidate_features], train_labels)\n",
    "            predictions = model.predict(test_data[:, candidate_features])\n",
    "            acc = accuracy_score(test_labels, predictions)\n",
    "\n",
    "            if acc >= best_accuracy:\n",
    "                best_accuracy = acc\n",
    "                feature_to_remove = feature\n",
    "\n",
    "        if feature_to_remove is not None:\n",
    "            selected_features.remove(feature_to_remove)\n",
    "            accuracies.append(best_accuracy)\n",
    "            num_features.append(len(selected_features))\n",
    "            print(f\"Removed feature: {feature_to_remove}, Accuracy: {best_accuracy:.4f}\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return selected_features, num_features, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603e78e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb2ad4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602fc1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad52854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f2e7ec4",
   "metadata": {},
   "source": [
    "# Forward Selection Algorithm for Feature Selection\n",
    "\n",
    "**Forward Selection** is a simple and intuitive **wrapper** method for feature selection. Unlike Backward Elimination, it starts with an empty set of features and iteratively adds the most significant features one by one until the best subset is found.\n",
    "\n",
    "## How Forward Selection Works:\n",
    "\n",
    "1. **Start with an empty feature set.**\n",
    "2. For each feature not yet selected, **train the model** (e.g., Na√Øve Bayes classifier) using the current selected features plus this candidate feature.\n",
    "3. **Evaluate the model's performance** using a chosen metric (such as accuracy).\n",
    "4. **Select the feature** that improves the model's performance the most.\n",
    "5. **Add this feature** to the selected feature set.\n",
    "6. **Repeat steps 2-5** until adding more features does not improve performance or a stopping criterion is met (e.g., maximum number of features).\n",
    "\n",
    "## Advantages:\n",
    "- Simple and efficient when the number of features is large.\n",
    "- Builds up the model incrementally, which can be easier to interpret.\n",
    "\n",
    "## Disadvantages:\n",
    "- May miss interactions between features since it only adds one feature at a time.\n",
    "- Still computationally expensive for very large feature sets.\n",
    "\n",
    "In this project, we will implement **Forward Selection** manually (without using any ready-made libraries) on the TinyMNIST dataset. For classification, we will use the Na√Øve Bayes optimal classifier from existing packages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40060c8d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
